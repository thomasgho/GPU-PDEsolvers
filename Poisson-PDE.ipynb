{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Poisson problem on CPU and GPU - The 5-point stencil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numba\n",
    "from numba import cuda\n",
    "import math\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the discrete Laplacian on CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following Poisson problem on the unit square $\\Omega = [0, 1]^2$. Solve\n",
    "\n",
    "$$\n",
    "-\\Delta u = 1\n",
    "$$\n",
    "\n",
    "in $\\Omega$ with $x = 0$ on $\\partial\\Omega$. To solve this problem we consider a discretisation based on the finite difference method. Let $x_i = ih$ with $h = \\frac{1}{N - 1}$ and $i=0, \\dots, N - 1$. Let $y_j$ be defined similarly. We want to compute approximate solutions $u$ at the points $(x_i, y_j)$. Hence, define\n",
    "\n",
    "$$\n",
    "u_{i,j} := u(x_i, y_j).\n",
    "$$\n",
    "\n",
    "We now approximate the value $-\\Delta u(x_i, y_j)$ by the following approximation.\n",
    "\n",
    "$$\n",
    "-\\Delta u_{i, j}\\approx \\frac{4u_{i, j} - u_{i - 1, j} - u_{i + 1, j} - u_{i, j - 1} - u_{i, j+ 1}}{h^2}\n",
    "$$\n",
    "\n",
    "for interior points $0 < i, j < N - 1$. On boundary points we impose the condition $u_{i, j} = 0$. With this discretisation the PDE problem becomes a linear matrix problem\n",
    "\n",
    "$$\n",
    "A x = f\n",
    "$$\n",
    "\n",
    "with $f$ being a vector of all ones for interior nodes and zero at the boundary and the vector $x\\in\\mathbb{R}^{N^2}$ collects all the values $u_{i, j}$ such that $u_{i, j} = x_{jN + i}$.\n",
    "\n",
    "We now want to implement the generation of the associated matrix. Let us calculate how many nonzero matrix elements we have. We have $(N - 2)^2$ interior points. Each row of $A$ associated with an interior point has $5$ entries. Furthermore, we have $4N - 4$ boundary points. Each boundary point requires a single diagonal entry in the matrix $A$. In total we therefore have\n",
    "\n",
    "$$\n",
    "5 (N^2 - 4N + 4) + 4N - 4 = 5N^2 - 16N + 16\n",
    "$$\n",
    "\n",
    "nonzero entries in the matrix $A$. The matrix $A$ has dimension $N^2$. Hence, there are in total $N^4$ elements. This means that the fraction of nonzero elements to overall elements is of order $O(N^{-2})$, or in other words. If we have $N = 100$ then the overall matrix dimension is $10,000$ and the number of nonzero elements is of the order of one tenthousands of the total number of matrix elements. The following is Python code that generates the matrix $A$ and the right-hand side $f$, and then solves the linear system and visualizes the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretise_poisson(N):\n",
    "    \"\"\"Generate the matrix and rhs associated with the discrete Poisson operator.\"\"\"\n",
    "    \n",
    "    nelements = 5 * N**2 - 16 * N + 16\n",
    "    \n",
    "    row_ind = np.empty(nelements, dtype=np.float32)\n",
    "    col_ind = np.empty(nelements, dtype=np.float32)\n",
    "    data = np.empty(nelements, dtype=np.float32)\n",
    "    \n",
    "    f = np.empty(N * N, dtype=np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    for j in range(N):\n",
    "        for i in range(N):\n",
    "            if i == 0 or i == N - 1 or j == 0 or j == N - 1:\n",
    "                row_ind[count] = col_ind[count] = j * N + i\n",
    "                data[count] =  1\n",
    "                f[j * N + i] = 0\n",
    "                count += 1\n",
    "                \n",
    "            else:\n",
    "                row_ind[count : count + 5] = j * N + i\n",
    "                col_ind[count] = j * N + i\n",
    "                col_ind[count + 1] = j * N + i + 1\n",
    "                col_ind[count + 2] = j * N + i - 1\n",
    "                col_ind[count + 3] = (j + 1) * N + i\n",
    "                col_ind[count + 4] = (j - 1) * N + i\n",
    "                                \n",
    "                data[count] = 4 * (N - 1)**2\n",
    "                data[count + 1 : count + 5] = - (N - 1)**2\n",
    "                f[j * N + i] = 1\n",
    "                \n",
    "                count += 5\n",
    "                                                \n",
    "    return coo_matrix((data, (row_ind, col_ind)), shape=(N**2, N**2)).tocsr(), f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the discrete Laplacian on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a GPU kernel, which given a one-dimensional array of values $u_{i,j}$ in the unit square grid, evaluates this discrete Laplace operator without explicity creating a matrix. Instead $N^2$ threads check if its associated with a boundary value or an interior value. If it is associated with a boundary value, the kernal writes the corresponding $u_{i,j}$ value in the result array (because our sparse matrix was just the identity for boundary values). If it is associated with an interior point, the kernal writes the value of evaluating the 5 point stencil for the corresponding interior point into the result array.\n",
    "\n",
    "We know that the first and last row in the 2D representation of $u_{i,j}$ correspond to boundry values. For (1D) thread position $x$, this condition can is satisfied if:\n",
    "- $x < (N - 1)$,  or \n",
    "- $N^2 - N < x \\leq N^2 - 1)$\n",
    "\n",
    "We also know that the first and last column in the 2D representation of $u_{i,j}$ correspond to boundry values. For (1D) thread position $x$, this condition can is satisfied if:\n",
    "- $x$ is divisible by $N$ (leftmost column), or \n",
    "- $x + 1$ is divisibile by $N$ (rightmost column)\n",
    "\n",
    "We know that the each point in the 2D representation of $u_{i,j}$ will have a corresponding a five point stencil in 1D given by: \n",
    "- $u(x)$ for the point itself\n",
    "- $u(x-1)$ for the point on its left\n",
    "- $u(x+1)$ for the point on its right\n",
    "- $u(x-N)$ for the point above it\n",
    "- $u(x+N)$ for the point below it\n",
    "\n",
    "The equation for $-\\Delta u_{i, j}$ can then be used to calculate non-boundry values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA implementation of discrete laplacian operator without shared memory (see next section for shared memory)\n",
    "\n",
    "@cuda.jit\n",
    "def discretise_poisson_gpu(vec_in, vec_out):\n",
    "    '''\n",
    "    Evaluate the linear matrix problem: Ax = f, \n",
    "    without the need to define the matrix A.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    vec_in  -- ndarray of shape [N^2,], corresponding to the solution at each point\n",
    "    vec_out  -- ndarray of shape [N^2,], corresponding to the interior nodes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    if i >= N:\n",
    "        return\n",
    "    if j >= N:\n",
    "        return\n",
    "    \n",
    "    # Compute the vector index\n",
    "    k = j * N + i\n",
    "\n",
    "    if i == 0 or i == N - 1 or j == 0 or j == N - 1:\n",
    "        # We are at the boundary\n",
    "        # Here, the matrix just acts like the identity\n",
    "        vec_out[k] = vec_in[k]\n",
    "        return\n",
    "\n",
    "    # Now deal with the interior element\n",
    "    up = vec_in[(j + 1) * N + i]\n",
    "    down = vec_in[(j - 1) * N + i]\n",
    "    left = vec_in[j * N + i - 1]\n",
    "    right = vec_in[j * N + i + 1]\n",
    "    center = vec_in[k]\n",
    "\n",
    "    vec_out[k] = (N - 1)**2 * (numba.float32(4) * center - up - down - left - right)\n",
    "\n",
    "    \n",
    "def eval_gpu(x):\n",
    "    \"Evaluate the discrete Laplacian on the GPU.\"\n",
    "\n",
    "    res = np.empty(N * N, dtype=np.float32)\n",
    "\n",
    "    nblocks = (N + 31) // 32\n",
    "    discretise_poisson_gpu[(nblocks, nblocks), (32, 32)](x.astype('float32'), res)\n",
    "    \n",
    "    return res.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we validate the above kernal against a pre-written CPU kernal. Given the problem attempts to solve:\n",
    "$$\n",
    "-\\Delta u = 1\n",
    "$$\n",
    "we expect the above kernal to return the array $f_{ij} = 1$ for all interior nodes and $f_{ij} = 0$ for boundry values. I.e. we expect $f$ (when reshaped to $N$ by $N$) to be the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "0  &  0  &  0  &\\ldots & 0\\\\\n",
    "0  &  1  &  1  &\\ldots & 0\\\\\n",
    "0  &  1  &  1  &\\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0  &   0  & 0     &\\ldots & 0\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "# Define padder (pad boundries with zeros)\n",
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 0)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "rand = np.random.RandomState(0)\n",
    "\n",
    "# Parameters of linear equation\n",
    "N = 1000\n",
    "omega = 0\n",
    "\n",
    "# Function to discretize\n",
    "u =  np.pad(rand.randn(N-2, N-2), 1, pad_with)\n",
    "    \n",
    "# Array of interior points\n",
    "u_tilda = u[1:N-1, 1:N-1].flatten()\n",
    "\n",
    "# CPU calculation\n",
    "A, _ = discretise_poisson(N)\n",
    "f_cpu_flat = A @ u.flatten()\n",
    "f_cpu_square = f_cpu_flat.reshape(N, N)\n",
    "\n",
    "# GPU calculation\n",
    "f_gpu_interior_flat = eval_gpu(u_tilda)\n",
    "f_gpu_interior_square = f_gpu_interior_flat.reshape(N-2, N-2)\n",
    "f_gpu_square = np.pad(f_gpu_interior_square, 1, pad_with)\n",
    "\n",
    "rel_error = np.linalg.norm(f_cpu_square.flatten() - f_gpu_square.flatten(), np.inf) / np.linalg.norm(f_cpu_square.flatten(), np.inf)\n",
    "print(f\"Relative error: {rel_error}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error is in the order of machine precision in 32 bit floating point accuracy. This is because on the CPU we are using double precision numbers and on the GPU we use single precision numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the 5-point stencil in CUDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large $N$ case is narrowly faster on the GPU than on the CPU. The problem is memory transfer. For interior points we have 6 accesses to global memory (5 reads, one write) and 6 floating point operations. Hence, the kernels run mostly memory bound. \n",
    "\n",
    "To not underutilize the device, we need to load more data than threads. We read blocks of data into shared memory that are spatially close by. The kernels within a workgroup can then locally work on these with much reduced memory accesses.\n",
    "\n",
    "We need to be careful of boundry conditions since the 5 point stencil structure reaches above, below, left and right of each point. Hence, we must first load data into the _interior_ threads of each block, i.e. we load data into the internal thread points: $[i+1][j+1]$. This will leave a *'halo'* surrounding each block, for which the rows and columns of adjescent blocks can be loaded in, ensuring the stencil calculation is always well defined for data centered on $[i+1][j+1]$.\n",
    "\n",
    "We also need to be careful setting the global boundry values explicitly to zero. That is, when the global thread position is on the mesh boundry, we load in zero. For each boundary point $x_k$ we always have $x_k = 0$. By doing this, we can therefore reduce the system to something that only acts on the interior grid points, giving us an operator $\\tilde{A}:\\rightarrow \\mathbb{R}^{(N-2)^2}\\rightarrow\\mathbb{R}^{(N-2)^2}$ where the vector $\\tilde{x}$ when we apply $\\tilde{A}\\tilde{x}$ only contains the interior points $u_{i, j}$ for $0 < i, j < N$.\n",
    "\n",
    "We also allow a larger class of possible PDE problems, namely so called modified Helmholtz problems of the form\n",
    "\n",
    "$$\n",
    "-\\Delta u + \\omega^2 u = f\n",
    "$$\n",
    "\n",
    "by introducing an aditional input parameter $\\omega$ (see Helmholtz notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA implementation of discrete laplacian operator with shared memory\n",
    "\n",
    "# Thread block size of interior points\n",
    "SX = 32 \n",
    "SY = 32\n",
    "\n",
    "# Total thread block size including 'halo' boundries\n",
    "haloSX = SX + 2\n",
    "haloSY = SY + 2\n",
    "\n",
    "@cuda.jit\n",
    "def discretise_poisson_gpu_2(u_tilda, f, omega, N):\n",
    "    '''\n",
    "    Evaluate the linear matrix problem: A_tilda @ u_tilda = f, \n",
    "    without the need to define the matrix A_tilda. Boundary values\n",
    "    are set to zero.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    u_tilda  -- ndarray of shape [(N-2)^2,], corresponding to\n",
    "                the solution at each interior point\n",
    "    f        -- ndarray of shape [(N-2)^2,], corresponding to\n",
    "                the interior nodes\n",
    "    \n",
    "    '''\n",
    "    # Define shared memory\n",
    "    local_u = cuda.shared.array((haloSX, haloSY), numba.float32)\n",
    "    \n",
    "    # Local thread position\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    \n",
    "    # Global thread position\n",
    "    pi, pj = cuda.grid(2)\n",
    "    \n",
    "    if pi >= N-2:\n",
    "        return\n",
    "    if pj >= N-2:\n",
    "        return\n",
    "    \n",
    "    # Compute the vector index\n",
    "    pk = pj * (N - 2) + pi\n",
    "    \n",
    "    # Load interior points of each block\n",
    "    local_u[i + 1, j + 1] = u_tilda[pk]\n",
    "    \n",
    "    # Load halo boundries of each block\n",
    "    if i == 0 and pi != 0:                          # Left edges\n",
    "        local_u[i, j + 1] = u_tilda[pk - 1]\n",
    "    if j == 0 and pj != 0:                          # Top edges             \n",
    "        local_u[i + 1, j] = u_tilda[pk - (N - 2)]\n",
    "    if i == SX - 1 and pi != (N - 2) - 1:           # Right edges\n",
    "        local_u[-1, j + 1] = u_tilda[pk + 1]\n",
    "    if j == SY - 1 and pj != (N - 2) - 1:           # Bottom edges\n",
    "        local_u[i + 1, -1] = u_tilda[pk + (N - 2)]\n",
    "\n",
    "    # Load global boundry conditions (boundry values = 0)\n",
    "    if pi == 0: \n",
    "        local_u[i, j + 1] = numba.float32(0)        # Global left edge\n",
    "    if pj == 0: \n",
    "        local_u[i + 1,j] = numba.float32(0)         # Global top edge\n",
    "    if pi == (N - 2) - 1:\n",
    "        local_u[i + 2, j + 1] = numba.float32(0)    # Global right edge\n",
    "    if pj == (N - 2) - 1:\n",
    "        local_u[i + 1, j + 2] = numba.float32(0)    # Global bottom edge\n",
    "\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Calculation of interior nodes in f\n",
    "    up = local_u[i + 2, j + 1]\n",
    "    down = local_u[i, j + 1]\n",
    "    left = local_u[i + 1, j]\n",
    "    right = local_u[i + 1, j + 2]\n",
    "    center = local_u[i + 1, j + 1]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "\n",
    "    f[pk] = (N - 1)**2 * (numba.float32(4) * center - up - down - left - right) + (omega**2) * center\n",
    "        \n",
    "\n",
    "def eval_gpu_2(u_tilda):\n",
    "    '''\n",
    "    Evaluate the discrete Laplacian on the GPU.\n",
    "    '''\n",
    "    \n",
    "    # Create empty array \n",
    "    f = np.empty((N-2)**2, dtype=np.float32)\n",
    "    \n",
    "    # Run GPU kernal\n",
    "    nblocks = ((N-2) + 31) // 32\n",
    "    discretise_poisson_gpu_2[(nblocks, nblocks), (32, 32)](u_tilda.astype('float32'), f, omega, N)\n",
    "    \n",
    "    return f.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following does some timing comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for kernal to evaluate with shared memory for N = 500:\n",
      "2.41 ms ± 25.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate with shared memory for N = 600:\n",
      "2.67 ms ± 46.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate with shared memory for N = 700:\n",
      "2.83 ms ± 32 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate with shared memory for N = 800:\n",
      "3.36 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate with shared memory for N = 900:\n",
      "3.64 ms ± 21.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for N in np.arange(500, 1000, 100):\n",
    "    print(f'Time for kernal to evaluate with shared memory for N = {N}:')\n",
    "    %timeit eval_gpu_2(u_tilda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for kernal to evaluate without shared memory for N = 500:\n",
      "3.44 ms ± 42.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate without shared memory for N = 600:\n",
      "3.75 ms ± 94.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate without shared memory for N = 700:\n",
      "3.98 ms ± 8.52 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate without shared memory for N = 800:\n",
      "4.24 ms ± 65.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Time for kernal to evaluate without shared memory for N = 900:\n",
      "4.47 ms ± 45 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for N in np.arange(500, 1000, 100):\n",
    "    print(f'Time for kernal to evaluate without shared memory for N = {N}:')\n",
    "    %timeit eval_gpu(u.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we see that for a range of different N values, the new GPU kernal offers faster evaluation time, which gets relatively faster and faster as N scales. This effectively shows that the issue with memory transfer is reduced, especially for large matricies (which scale with N^2, where memory transfer would increase more)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
